# -*- coding: utf-8 -*-
"""detr_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb

# Object Detection with DETR - a minimal implementation

In this notebook we show a demo of DETR (Detection Transformer), with slight differences with the baseline model in the paper.

We show how to define the model, load pretrained weights and visualize bounding box and class predictions.

Let's start with some common imports.
"""

# Add missing typing imports if not present
from typing import List, Any
from detectors.base_detector import BaseDetector
from detectors.detr_detector_demo import DETRdemo
# Commented out IPython magic to ensure Python compatibility.
from PIL import Image
import requests
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'retina'

import torch
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
torch.set_grad_enabled(False);

"""## DETR
Here is a minimal implementation of DETR:
"""

class DetrDetector(BaseDetector):
    """
    DETR Detector implementation.
    """
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.num_classes = cfg.num_classes
        self.hidden_dim = getattr(cfg, "hidden_dim", 256)
        self.nheads = getattr(cfg, "nheads", 8)
        self.num_encoder_layers = getattr(cfg, "num_encoder_layers", 6)
        self.num_decoder_layers = getattr(cfg, "num_decoder_layers", 6)
        self.device = cfg.device
        # Model will be loaded in load_model()
        self.model = None

    def load_model(self):
        """
        Load the DETR model and prepare it for inference.
        """
        # Use out-of-the-box COCO weights (91 classes)
        coco_num_classes = 91
        self.model = DETRdemo(
            coco_num_classes,
            hidden_dim=self.hidden_dim,
            nheads=self.nheads,
            num_encoder_layers=self.num_encoder_layers,
            num_decoder_layers=self.num_decoder_layers,
        )
        state_dict = torch.hub.load_state_dict_from_url(
            url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
            map_location='cpu', check_hash=True)
        # Load pretrained weights, ignoring mismatched classification head
        self.model.load_state_dict(state_dict, strict=False)
        self.model.eval();        
        # Move the model to the correct device
        device = torch.device(f"cuda:{self.cfg.device}" if getattr(self.cfg, "device", None) is not None else "cpu")
        self.model.to(device)
        self.model.eval()

    def infer(self, x: torch.Tensor, target: torch.Tensor, bboxes: torch.Tensor, batch_size: int = 1) -> torch.Tensor:
        """
        Run model inference for loss computation.

        Args:
            x: Input image tensor of shape (C, H, W) or (N, C, H, W)
            target: Target class labels (LongTensor) of shape (N,)
            bboxes: Ground truth bounding boxes (FloatTensor) of shape (N, 4)
            batch_size: Batch size (unused for DETR; included for interface consistency)

        Returns:
            torch.Tensor: Scalar loss value computed by the model's criterion.
        """
        # Switch model to training mode for loss computation
        self.model.train()
        device = next(self.model.parameters()).device
        # Ensure batch dimension
        if x.dim() == 3:
            x = x.unsqueeze(0)
        x = x.to(device)
        # Build list of target dicts for DETR's criterion
        targets = []
        for i in range(x.shape[0]):
            targets.append({
                "labels": target[i].to(device).long(),
                "boxes": bboxes[i].to(device).float()
            })
        # Forward pass: get raw outputs
        outputs = self.model(x)
        # Compute losses using the model's built-in criterion
        if hasattr(self.model, "criterion"):
            loss_dict = self.model.criterion(outputs, targets)
        else:
            # If criterion attached externally, call a standalone function
            loss_dict = criterion(outputs, targets)
        # Sum all loss terms into a single scalar
        total_loss = sum(loss for loss in loss_dict.values())
        return total_loss

    def preprocess_input(self, image_path: str) -> dict:
        """
        Convert an image path into a DETR-compatible input dictionary.
        Loads the image, applies resizing and normalization, and returns:
            {
                "image": Tensor of shape (1, 3, H, W),
                "height": original image height,
                "width": original image width
            }
        """
        # Load image and convert to RGB
        img = Image.open(image_path).convert("RGB")
        # Prepare transforms: resize shorter side to 800 pixels, then normalize
        transform = T.Compose([
            T.Resize(800),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])
        # Apply transforms and add batch dimension
        img_tensor = transform(img).unsqueeze(0).to(self.device)
        # Return tensor and original dimensions
        return {
            "image": img_tensor,
            "height": img.height,
            "width": img.width
        }

    def zero_grad(self):
        """
        Zero out gradients of the DETR model.
        """
        if self.model is not None:
            self.model.zero_grad()


    def predict_and_save(
        self,
        image: torch.Tensor,
        path: str,
        target: int = None,
        untarget: int = None,
        is_targeted: bool = True,
        threshold: float = 0.7,
        format: str = "RGB",
        gt_bbox: List[int] = None,
        result_dict: bool = False,
        image_id: int = None
    ) -> Any:
        # Run DETR inference and save visualization
        import torch as ch
        from PIL import ImageDraw
        import torchvision.transforms as T
        self.model.eval()
        device = next(self.model.parameters()).device
        with ch.no_grad():
            # Convert input tensor to PIL for drawing
            image_np = (image.detach().cpu().clamp(0,1).permute(1,2,0).numpy() * 255).astype("uint8")
            pil_img = Image.fromarray(image_np)
            # Prepare input batch for DETR
            inp = {
                "image": T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(
                            T.ToTensor()(pil_img)
                        ).unsqueeze(0).to(device)
            }
            # Extract tensor and call model positionally (DETRdemo.forward expects a single tensor)
            image_tensor = inp["image"]
            outputs = self.model(image_tensor)
            # logits: [1, num_queries, num_classes+1], boxes: [1, num_queries, 4]
            logits = outputs["pred_logits"][0]
            boxes = outputs["pred_boxes"][0]
            # Compute probabilities and filter by threshold
            probs = logits.softmax(-1)[..., :-1]  # drop "no object" class
            scores, labels = probs.max(-1)
            keep = scores > threshold
            scores = scores[keep]
            labels = labels[keep]
            boxes = boxes[keep]
            # Convert boxes from [cx, cy, w, h] (normalized) to [x1, y1, x2, y2] in pixels
            W, H = pil_img.size
            cx, cy, w, h = boxes.unbind(-1)
            x1 = (cx - 0.5 * w) * W
            y1 = (cy - 0.5 * h) * H
            x2 = (cx + 0.5 * w) * W
            y2 = (cy + 0.5 * h) * H
            boxes_xyxy = ch.stack([x1, y1, x2, y2], dim=-1).cpu().tolist()
        # Draw predictions
        draw = ImageDraw.Draw(pil_img)
        pred_classes, pred_confs, pred_boxes = [], [], []
        for lbl, sc, box in zip(labels.tolist(), scores.tolist(), boxes_xyxy):
            x1, y1, x2, y2 = [int(coord) for coord in box]
            draw.rectangle([x1, y1, x2, y2], outline="red", width=2)
            class_name = self.resolve_label_index(lbl)
            draw.text((x1, y1 - 10), f"{class_name}, {sc:.2f}")
            pred_classes.append(lbl)
            pred_confs.append(sc)
            pred_boxes.append(box)
        pil_img.save(path)

        # Evaluate targeted/untargeted criteria
        # Compute IoUs against gt_bbox if provided
        best_class = None
        closest_confidence = None
        best_iou = None
        formatted_gt_bbox = [round(gt_bbox[0],1), round(gt_bbox[1],1),
                                round(gt_bbox[2]-gt_bbox[0],1), round(gt_bbox[3]-gt_bbox[1],1)]
        if gt_bbox is not None and pred_boxes:
            gt = ch.tensor(gt_bbox, dtype=ch.float32)
            dets = ch.tensor(pred_boxes, dtype=ch.float32)
            # convert gt to [x1,y1,x2,y2]
            # IoU computation (use box_iou from ultralytics or implement manually)
            from ultralytics.utils.metrics import box_iou
            ious = box_iou(dets, gt.unsqueeze(0)).squeeze(1)
            best_idx = ious.argmax().item()
            best_iou = float(ious[best_idx].item())
            best_class = pred_classes[best_idx] if best_iou > 0.5 else None
            closest_confidence = pred_confs[best_idx] if best_iou > 0.5 else None
            target_pred_exists = (best_iou > 0.5 and best_class == target)
            untarget_pred_not_exists = not (best_iou > 0.5 and best_class == untarget)
        else:
            target_pred_exists = target in pred_classes if target is not None else False
            untarget_pred_not_exists = all(lbl != untarget for lbl in pred_classes)

        meets = ((is_targeted and target_pred_exists and (untarget is None or untarget_pred_not_exists))
                 or (not is_targeted and untarget_pred_not_exists))

        if result_dict:
            # Assemble result dict following yolov3 style
            result = {
                "detections": [
                    {"image_id": image_id or -1,
                     "category_id": lbl,
                     "bbox": [round(box[0],1), round(box[1],1),
                              round(box[2]-box[0],1), round(box[3]-box[1],1)],
                     "score": sc}
                    for lbl, sc, box in zip(pred_classes, pred_confs, pred_boxes)
                ],
                "closest_class": best_class if gt_bbox is not None else None,
                "closest_class_name": self.resolve_label_index(best_class) if best_class is not None else None,
                "closest_category_id": self.yolov3_resolve_label_index(self.resolve_label_index(best_class)) if best_class is not None else None, # get 80 class index mapping from YOLO, since DETR mapped to 90
                "closest_confidence": closest_confidence if gt_bbox is not None else None,
                "best_iou": best_iou if gt_bbox is not None else None,
                "gt_bbox": [float(x) for x in formatted_gt_bbox] if gt_bbox is not None else None,
                "target_pred_exists": target_pred_exists,
                "untarget_pred_not_exists": untarget_pred_not_exists
            }
            return meets, result
        return meets

    def resolve_label_index(self, label_name: str) -> int:
        # DETR uses COCO indexing: map human-readable to index or vice versa
        coco_classes = [
            'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
            'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
            'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
            'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
            'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
            'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
            'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
            'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
            'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]
        if isinstance(label_name, int):
            return coco_classes[label_name]
        # else string -> index
        label_norm = label_name.strip().lower()
        lookup = {name: idx for idx, name in enumerate(coco_classes)}
        if label_norm in lookup:
            return lookup[label_norm]
        raise ValueError(f"Label '{label_name}' not found in DETR COCO classes.")

    def yolov3_resolve_label_index(self, label):
        def normalize(name):
            return name.replace('_', ' ').lower()
        coco_class_names = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',
            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter',
            'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
            'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',
            'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
            'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',
            'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',
            'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',
            'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',
            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
            'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]
        if isinstance(label, int):
            if 0 <= label < len(coco_class_names):
                return coco_class_names[label]
            raise ValueError(f"Class index {label} is out of bounds.")
        label = normalize(label)
        label_lookup = {normalize(name): idx for idx, name in enumerate(coco_class_names)}
        if label not in label_lookup:
            raise ValueError(f"Label '{label}' not found in YOLOv3 COCO class list.")
        return label_lookup[label]


# """As you can see, DETR architecture is very simple, thanks to the representational power of the Transformer. There are two main components:
# * a convolutional backbone - we use ResNet-50 in this demo
# * a Transformer - we use the default PyTorch nn.Transformer


# Let's construct the model with 80 COCO output classes + 1 ⦰ "no object" class and load the pretrained weights.
# The weights are saved in half precision to save bandwidth without hurting model accuracy.
# """

# detr = DETRdemo(num_classes=91)
# state_dict = torch.hub.load_state_dict_from_url(
#     url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
#     map_location='cpu', check_hash=True)
# detr.load_state_dict(state_dict)
# detr.eval();

# """## Computing predictions with DETR

# The pre-trained DETR model that we have just loaded has been trained on the 80 COCO classes, with class indices ranging from 1 to 90 (that's why we considered 91 classes in the model construction).
# In the following cells, we define the mapping from class indices to names.
# """

# # COCO classes
# CLASSES = [
#     'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
#     'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
#     'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
#     'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
#     'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
#     'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
#     'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
#     'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
#     'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
#     'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
#     'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
#     'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
#     'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
#     'toothbrush'
# ]

# # colors for visualization
# COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
#           [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

# """DETR uses standard ImageNet normalization, and output boxes in relative image coordinates in $[x_{\text{center}}, y_{\text{center}}, w, h]$ format, where $[x_{\text{center}}, y_{\text{center}}]$ is the predicted center of the bounding box, and $w, h$ its width and height. Because the coordinates are relative to the image dimension and lies between $[0, 1]$, we convert predictions to absolute image coordinates and $[x_0, y_0, x_1, y_1]$ format for visualization purposes."""

# # standard PyTorch mean-std input image normalization
# transform = T.Compose([
#     T.Resize(800),
#     T.ToTensor(),
#     T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
# ])

# # for output bounding box post-processing
# def box_cxcywh_to_xyxy(x):
#     x_c, y_c, w, h = x.unbind(1)
#     b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
#          (x_c + 0.5 * w), (y_c + 0.5 * h)]
#     return torch.stack(b, dim=1)

# def rescale_bboxes(out_bbox, size):
#     img_w, img_h = size
#     b = box_cxcywh_to_xyxy(out_bbox)
#     b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
#     return b

# """Let's put everything together in a `detect` function:"""

# def detect(im, model, transform):
#     # mean-std normalize the input image (batch-size: 1)
#     img = transform(im).unsqueeze(0)

#     # demo model only support by default images with aspect ratio between 0.5 and 2
#     # if you want to use images with an aspect ratio outside this range
#     # rescale your image so that the maximum size is at most 1333 for best results
#     assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'

#     # propagate through the model
#     outputs = model(img)

#     # keep only predictions with 0.7+ confidence
#     probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
#     keep = probas.max(-1).values > 0.7

#     # convert boxes from [0; 1] to image scales
#     bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)
#     return probas[keep], bboxes_scaled

# """## Using DETR
# To try DETRdemo model on your own image just change the URL below.
# """

# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
# im = Image.open(requests.get(url, stream=True).raw)

# scores, boxes = detect(im, detr, transform)

# """Let's now visualize the model predictions"""

# def plot_results(pil_img, prob, boxes):
#     plt.figure(figsize=(16,10))
#     plt.imshow(pil_img)
#     ax = plt.gca()
#     for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):
#         ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
#                                    fill=False, color=c, linewidth=3))
#         cl = p.argmax()
#         text = f'{CLASSES[cl]}: {p[cl]:0.2f}'
#         ax.text(xmin, ymin, text, fontsize=15,
#                 bbox=dict(facecolor='yellow', alpha=0.5))
#     plt.axis('off')
#     plt.show()

# plot_results(im, scores, boxes)

# ### INSERT DETR BASE_DETECTOR IMPLEMENTATION BELOW HERE:

